# 第四章 线性回归&过拟合&正则化

## 1. 回顾

- 我们的目标（监督学习中）：
    - 学习一系列的判别函数/决策函数
    - 扩展：是否还能回忆起决策函数，损失函数，目标函数三个术语之间的区别和使用阶段。
- 贝叶斯网络
    - 利用训练数据求出先验概率和似然
    - 再根据贝叶斯公式利用先验概率和似然去计算后验概率，作为判别函数
- 能否直接从训练数据中获得判别函数呢？即不需要概率估计的过程

## 2. 回归和分类

- 两者都是监督学习方法
    - 目标：学习一个x到y的映射
- 对于分类问题而言，y是一个离散变量
- 对于回归问题而言，y是一个真实且连续的变量

## 3. 线性模型

- 样本：$x\in R^d,x=[x_1,x_2,...,x_d]^T$
- 设置权重参数: $w=[w_1,w_2,...,w_d]^T \in R^d,b$
- 找到线性超平面函数:  $f(x)=w^Tx+b$
- 将参数b融入参数w中优化超平面函数表现形式:
    - $f(x)=w^Tx$
    - $x=[x_1,x_2,...,x_d,1]^T \in R^{d+1}$
    - $w=[w_1,w_2,...w_d,b]^T\in R^{d+1}$
- 多项式拟合：x的多项式形式可以拟合不同分布的点，多项式在样本空间中的表现形式可以是直线，可以是曲线。
    - 具体形式: $f(x,w)=w_0+w_1x+w_2x^2+...+w_Mx^M=\sum^M_{j=0}w_jx^j$
    - 以三阶多项式的表现形式为例：
    
    ![Untitled](%E7%AC%AC%E5%9B%9B%E7%AB%A0%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92&%E8%BF%87%E6%8B%9F%E5%90%88&%E6%AD%A3%E5%88%99%E5%8C%96%2017f840d193d84f2fa60572e57b84ee53/Untitled.png)
    
- 平方和损失函数(Sum of Squares Loss fuction)
    - 公式表示形式：
    
    $$
    MSE(w)=\frac{1}{n}\sum^n_{i=1}(y_i-f(x_i,w))^2
    $$
    
    - 目标：我们要寻找一组最优的w和b，使得MSE损失函数的值最小。
- 优化达到上述目标的方法
    - 最小二乘法（仅能用于线性回归及其变种）
        - $Loss(w)=(y-X^Tw)^T(y-X^Tw)$
        - 计算梯度/导数值: $\triangledown J=-2X(y-X^Tw)$
        - 令梯度为0，得到极值（注意极值不一定是最值，所以其实有时候优化到达的是极值点，又称局部最优点）：
            - $XX^Tw=Xy$
            - $w=(XX^T)^{-1}Xy$
        - 图形化描述：$x_1$和$x_2$组成的线性空间中，能够预测出距离$y$最近的$y^{head}$的值，就是$y$垂直线性空间做投影所与线性空间的交点。
            
            ![Untitled](https://github.com/mura1n/Machine-Learning-in-Practice-Crash-Course-Notes/blob/main/notes/week04_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%26%E8%BF%87%E6%8B%9F%E5%90%88%26%E6%AD%A3%E5%88%99%E5%8C%96/Untitled.png)
            
    - 梯度下降(Gradient Descent)
        
        ![Untitled](https://github.com/mura1n/Machine-Learning-in-Practice-Crash-Course-Notes/blob/main/notes/week04_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%26%E8%BF%87%E6%8B%9F%E5%90%88%26%E6%AD%A3%E5%88%99%E5%8C%96/Untitled 1.png)
        
        - 梯度下降能够找到一个局部最优解，对于非凸函数来说，无法确保这个局部最优解是全局最优解。
        - 下降的方向是根据梯度的反方向确定的，因为梯度的方向是函数值上升最快的方向，其反方向是函数值下降最快的方向，目的是让损失函数值降到最低。
        - 注意：决定步频的是学习率，学习率太大，会导致函数值在局部最优解之间来回徘徊；学习率太小，会导致到达局部最优解需要的迭代次数太多。所以，寻找合适的学习率十分重要。
        - 存在的问题：时间复杂度太大(O(ndt))，每一个数据点的每一个参数都需要移动，但是每一个参数的移动是必须的，所以要解决这个问题可以从遍历每一个数据这个方向解决。(解释：n是样本个数，d是特征维度，t是迭代的次数)
            - 随机梯度下降(Stochastic Gradient Descent, SGD):
                - 随机的从数据中拿b个数据
                - 时间复杂度为O(bdt)
- 凸优化
    - 最小平方误差被称为凸问题，解决凸问题的方法叫做凸优化。
    - 方法距离：
        - 梯度下降(Gradient Descent): 一阶导
        - 牛顿法(Newton’s method): 二阶导
        - Quasi Newton’s methods:等
    - 注：对于非凸问题都只能找到局部最优解。

## 4. 过拟合

- 问题的产生：通过增加模型复杂度会发现训练集和测试集的误差不是同步变化的，当复杂度比较大时，测试集的误差会不降反增。（这种问题称之为过拟合）
    
    ![Untitled](%E7%AC%AC%E5%9B%9B%E7%AB%A0%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92&%E8%BF%87%E6%8B%9F%E5%90%88&%E6%AD%A3%E5%88%99%E5%8C%96%2017f840d193d84f2fa60572e57b84ee53/Untitled%203.png)
    
- 岭回归(Ridge Regression)
    - 岭回归发现随着多项式复杂度提高，参数在不断变大。所以岭回归想到去控制参数的大小，其认为模型在拟合不同的点时曲线的过渡应该是平滑的。
    - 岭回归给参数加上了L2惩罚项，可以发现如果损失函数要减小，那么需要w越大，但是相应的$w^2$的值也会越大，导致损失函数的整体又会增大，所以加上惩罚项之后会防止参数出现过大的情况。（$\lambda$是一个需要我们调节的超参数，在机器学习中，需要人为去调节的参数叫做超参数）
    
    $$
    w^*=argmin(\sum^{n}_{i=1}(y_i-x^T_iw)^2+\lambda\sum^{p}_{j=1}w^2_j)
    $$
    
- 正则化理解：正则化包含了我们对模型的一些先验认知，比如我们可能认为模型的过渡是平滑的，不会出现陡峭的变化；模型的参数不能很大，要在我们的预期范围内等等。所以我们会用先验认知来限制模型，正则化是一把双刃剑，它能够帮我们过滤掉不好的模型，同时也可能漏掉一部分好的模型。
- 对于超参数的观察（$\lambda$的取值也能影响模型的复杂度，取值越小，模型的复杂度越大，模型越复杂；取值越大，模型的复杂度越小，模型越简单，下面以9阶多项式为例说明）：
    
    ![Untitled](%E7%AC%AC%E5%9B%9B%E7%AB%A0%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92&%E8%BF%87%E6%8B%9F%E5%90%88&%E6%AD%A3%E5%88%99%E5%8C%96%2017f840d193d84f2fa60572e57b84ee53/Untitled%204.png)
    
    - 横轴初始点~-35的区域称之为过拟合区域：此时模型过于复杂，所以在训练集上的表现很好，Training Loss很小，在测试集上的表现很差，Test Loss很大。
    - -25~横轴结束点：此时模型过于简单，所以在训练集和测试集上的表现都在变差，Training Loss和Test Loss均在增大。

## 5. 偏差和方差的分解

这一部分个人认为讲的比较抽象，所以建议看沐神的讲解：[视频链接]([https://www.bilibili.com/video/BV1H44y1v7Kf/?spm_id_from=333.337.search-card.all.click&vd_source=e6f8d61d66ba327ea1c77781c087068f](https://www.bilibili.com/video/BV1H44y1v7Kf/?spm_id_from=333.337.search-card.all.click&vd_source=e6f8d61d66ba327ea1c77781c087068f))

- 偏差(Bias)：每一次模型训练很多个点的均值与真实值之间的区别。（预测点和真实值的比较）
- 方差(Variance)：每一次训练出来的模型预测的点和每一次训练出来的模型预测的所有点的平均值之间的差别。（预测点和预测点平均值间的比较）
- 模型的泛化误差可以写为

$$
E_D[(y-f^{head}(x)^2)]=Bias[f^{head}]^2+Var[f^{head}]+\sigma^2
$$

- 欠拟合：高偏差，低方差（简单理解是高偏差，低方差的模型往往是复杂度比较低的模型，这类模型往往在训练集的拟合效果就不好，所以属于欠拟合）
- 过拟合：高方差，低偏差（简单理解是低偏差，高方差的模型往往是复杂度比较高的模型，这类模型往往在训练集的拟合效果很好，在测试集上效果很差，所以属于过拟合）
- 实际应用过程中：偏差大小看训练集上的误差即可，方差大小看训练集和测试集上误差之间的间隔距离

## 6. 如何避免欠拟合与过拟合

- 欠拟合的解决思路：降低偏差
    - 添加更多特征
    - 使用更加复杂的模型
    - 减小正则项
- 过拟合的解决思路：降低方差
    - 减少一些特征
    - 使用更加简单的模型
    - 添加更多正则项
    - 增加更多的数据

## 7. 如何衡量超参数选择的好坏

在模型训练完成之后，我们会在测试集上查看模型实际的效果，但是我们缺乏一个选择超参数的数据集。训练集的重点在于降低偏差，无法准确的衡量偏差和方差一起的情况；测试集由于是验证模型好坏的数据集，在使用测试集之前，不能让模型接触测试集，所以要另寻他法。

分为训练，验证和测试三个数据集：

- 训练集用于训练模型
- 验证集用于做超参数决策，选择模型和筛选参数
- 测试集用来验证最终的模型效果
- 五折交叉验证目前用的很少所以不予说明
