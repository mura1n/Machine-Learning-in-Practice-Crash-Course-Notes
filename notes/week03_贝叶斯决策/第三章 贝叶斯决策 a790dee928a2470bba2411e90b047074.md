# 第三章 贝叶斯决策

## 1. 贝叶斯理论研究的问题

- x：样本
- y：类别标签
- P (y | x)：在给定x的条件下，y发生的概率，重点也就是求这个条件概率取值。

## 2. 贝叶斯理论相关的定义

- 条件概率：

$$P(A|B)=P(A,B)/P(B)$$

- 若A与B相互独立，则联合概率可以表示为：

$$P(A,B)=P(A)P(B)$$

- 贝叶斯定理：

$$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$$

- 先验概率(Prior)：反映了我们关注的标签在自然界中(无人为干预的情况下)的数量分布情况（在某个特征下也可以）。例如：我们可以看在lightnes特征条件下See bass和salmon这两个标签的数量情况。更简单的来讲，先验就是在我们不知情的情况下猜测的标签种类。（我们倾向于猜测炸弹不会爆炸，因为其先验概率很小）
    - 如果没有先验概率的情况下，我们可能会认为salmon和sea bass的捕捉概率是相等的。这种假设并不适用于任何情况。
    
    $$P(y_{1} )=P(y_{2} )$$
    
    - 如果是二分类问题
    
    $$P(y_{1} )+P(y_{2} )=1$$
    
    - 如果只根据先验信息进行决策
        - 即如果y1的概率大于y2则认为是y1，否则就是y2.
        - 先验概率不一定准确，没有用到事物本身的feature，纯在猜测。
    
- 概率和似然的理解
    - 概率：在一件事的结果未知的情况下，通过事件自身的性质估计事件各个结果的可能性的大小，就是事件各个结果发生的概率。（抛硬币：硬币有两面，所以两面分别朝上的概率都是百分之五十，概率只有在事件发生前是有意义的，因为当硬币抛出后，结果就已经确定了。）
    - 似然：基于事件已经确定的结果来推测产生这个结果的可能环境（环境中的某些参数）。（抛硬币：直接抛硬币10000次，其中8000次正面朝上，2000次反面朝上，我们会认为硬币的构造比较特殊，进而推测该硬币的具体参数）
    - 如果认为θ是环境对应的参数，x是事件发生的结果。
        - 概率：
            
            $$P(x|θ)$$
            
        - 似然：
            
            $$L(θ|x)$$
            
- 极大似然估计：利用已知的样本标记结果，反推最有可能或者最大概率导致这些样本结果出现的模型参数。
    - 还是拿抛硬币实验举例，我们要得到概率，但我们并不知道θ是多少，需要根据观测结果进行推断。所以我们进行抛硬币实验，并记录抛出的结果序列，
        
        $$P(人像朝上)=θ$$
        
        $$P（人像朝下)=1-θ$$
        
    - 如果得到的序列中七次人像朝上，三次人像朝下，则L(θ)可以表示为
    
    $$L(θ)=θ^{7} (1-θ)^{3}$$
    
    - 枚举θ的值，并画出函数L(θ)的图像，发现函数在θ取0.7时取得最大值（最大似然估计取值），最可能发生七次人像朝上，三次人像朝下的情况。
        
        ![Untitled](https://github.com/mura1n/Machine-Learning-in-Practice-Crash-Course-Notes/blob/main/notes/week03_%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96/Untitled.png)
        
- 后验概率(Posterior)
    - 贝叶斯定律
        
        $$P(y_{i}|x)=\frac{P(x|y_{i})P(y_{i})}{P(x)}$$
        
        $$P(x)=\sum_{i=1}^{k}P(x|y_{i})P(y_{i})$$
        
    - 后验概率Posterior=(似然Likelihood×先验概率Prior)/Evidence
    - 后验概率与似然和先验概率的乘积成正比
    - Evidence（认为是P(x)）可以认为是一个常数，其作用是将后验概率求和得到1

## 3. 优化贝叶斯决策

- 如何判断标签的分类，如果P(y1|x) > P(y2|x)成立则为y1，反之则为y2。
- 贝叶斯决策能最小化出错的概率，这就是优化的由来，参考内容有两个，上面是例子，下面是理论。（[https://blog.csdn.net/Harry_Jack/article/details/111242672](https://blog.csdn.net/Harry_Jack/article/details/111242672)）（[https://blog.csdn.net/2301_79449205/article/details/134646696](https://blog.csdn.net/2301_79449205/article/details/134646696)）
- 推广到多个特征，多个类别的数据，每个分类的错误率不是同等重要的，怎么解决？

## 4. 贝叶斯风险

这部分不想做过多的说明，我的理解是，如果将两个类别互相识别错误的风险相当的前提下，类别只需要比较两者的后验概率即可，就与之前绘制直方图类似。如果两者不相等，则需要依据风险函数比较大小进行类别判定。下面直接罗列课程等式：

$$E_{i j}=E\left(\widehat{y_{i}} \mid y_{j}\right)$$

$$R\left(\widehat{y_{1}} \mid x\right)=E_{11} P\left(y_{1} \mid x\right)+E_{12} P\left(y_{2} \mid x\right)=E_{12} P\left(y_{2} \mid x\right)$$

$$R\left(\widehat{y_{2}} \mid x\right)=E_{21} P\left(y_{1} \mid x\right)+E_{22} P\left(y_{2} \mid x\right)=E_{21} P\left(y_{1} \mid x\right)$$

- 例子1（二分类决策）：如果两个类别的似然比值超过某个阈值，我们就采取决策，决定某个类别。
- 例子2（多分类决策）：对于多分类而言，我们将下式中i=j的E认为是0（正确），i≠j的E认为是1（错误）。其实不用管E的取值，这里限定E的取值只是让上述三个式子中的后两个式子的中间过程E11=0和E22=0，都得到最后的部分：
    
    $$E\left(\widehat{y_{i}} \mid y_{j}\right)$$
    
    然后胡神写的是在E同等重要的情况下（也即E12=E21），最小化风险函数Risk就是最大化后验概率P(yi|x)，不仔细看下标你可能会懵逼，你会觉得要让等式左边的R越小，等式右边的P不是也该越小吗？为什么要最大化呢？仔细看下表你就会发现R中的y head的下标和P中的y的下标是不一样的，你要y1 head的Risk值越小，y2对应的P值就越小，y1对应的P值就应该越大，所以确实是given x条件下y1的后验概率越大，这也是我当时困惑的地方，希望你能够理解。
    
- 来个总结：
    - 如果given x前提下yi发生的后验概率大于其它所有given x前提下yj发生的后验概率时，决策认为类别是yi
        
        ![Untitled](https://github.com/mura1n/Machine-Learning-in-Practice-Crash-Course-Notes/blob/main/notes/week03_%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96/Untitled%201.png)
        
    - 简化一下概念，每一个g_(x)都是一个分类器，每个分类器会接收每一个特征取值，最终分类会根据每一个分类器的取值做决策。（注：分类器的形式是多变的，可以是先验概率，可以是后验概率，也可以是似然等，总之可以是任意的function）
        
        ![Untitled](https://github.com/mura1n/Machine-Learning-in-Practice-Crash-Course-Notes/blob/main/notes/week03_%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96/Untitled%202.png)
        

## 5. 决策边界（Decision Regions and Surfaces）

大量的特征构成了一个特征空间，任何形式的决策（训练过程）都是将这个特征空间划分成很多个子空间，每个子空间中都有一定的样本(sample)，这些样本对应了一个标签(label)。如图，每个颜色的区域都是一个子空间，子空间的颜色代表了label，空间中的黑点就是sample。

![Untitled](https://github.com/mura1n/Machine-Learning-in-Practice-Crash-Course-Notes/blob/main/notes/week03_%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96/Untitled%203.png)

## 6. 回顾一下吧

- 贝叶斯的框架
    - 知道先验概率P(yi)，知道似然P(x|yi)，我们就可以得到一个最优的分类器。
    - 现实生活中，很难获取到准确的似然的信息（特征维度太高或者特征并不充分）。
    - 常用的做法：利用训练数据去估计出先验概率和似然，再去做贝叶斯决策。
- 离散形式极大似然估计
    
    ![Untitled](https://github.com/mura1n/Machine-Learning-in-Practice-Crash-Course-Notes/blob/main/notes/week03_%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96/Untitled%204.png)
    
    - 先验概率：将频率估计为概率，比如P(Evade=No)=7/10=0.7, P(Evade=Yes)=3/10=0.3。
    
    $$P\left(y_{k}\right)=\frac{N_{y_{k}}}{N}$$
    
    - 似然：在类别为yk的样本中特征为xi样本的占比。比如：P(Status=Married|No) = 4/7
        
        $$P\left(x_{i} \mid y_{k}\right)=\frac{\left|x_{i k}\right|}{N_{y_{k}}}$$
        
- 连续形式极大似然估计
    - 处理方式：
        - 将连续数据分为多个数据段，对于体重来说，可以50-100kg，100-150kg，150-200kg。再将数据段离散成不同的类别即可。
        - 暴力分为两个段，设置一个中间值，小于中间值的设为一类，大于中间值的设为另一类。
        - 直接去估计出现的概率（只能处理离散数据模型效果最好的方式，后面会讲到，我也不知道，忘了）

## 7. 朴素贝叶斯

- 贝叶斯决策遗留问题：估计数据最大的问题是数据的维度过大，以至于难以估计到似然。所以我们要提出假设解决这个问题，朴素贝叶斯应运而生。
- 朴素贝叶斯具体做法：
    - 给了一系列的特征x，目标是预测y所属的类别。更加具体地说，我们要找到一个y的值(分类)能够使后验概率p(y|x)最大。
    - 虽然我们知道p(y|x)与先验概率和似然的乘积成正比，但是似然很难获取。
    - 我们假设，每一个特征xi的似然是独立的。
        
        $$P\left(x_{1}, \cdots x_{p} \mid y\right)=P\left(x_{1} \mid y\right) \cdots P\left(x_{p} \mid y\right)$$
        
        - 存在问题1：浮点数连乘之后向下溢出消失。
        - 解决方法：取对数log，因为log是单调的，所以取对数后不会改变似然的相对大小。
        - 存在问题2：出现0值，连乘后还是0.
        - 解决方法：平滑处理(smoothing)，不让似然值为0.
            
            $$P\left(x_{i} \mid y_{k}\right)=\frac{\left|x_{i k}\right|+1}{N_{y_{k}}+K}$$
            
    - 优点：
        - 减少噪声数据的干扰
        - 处理缺失值的影响（概率估计）
        - 对无关的特征是具有鲁棒性的
        - 计算量小(只有乘法)
    - 缺点：
        - 上述假设在实际中可能并不成立
    - 垃圾邮件分类中为什么朴素贝叶斯表现很好：
        - 很多单词是相互独立的
        - 即使假设并不成立，但在某些情况下特征相对独立的性质是可以保留的，特征缩放后，数据的数值关系和顺序都保留下来了
        - 即使特征很多，计算开销仍然足够小
